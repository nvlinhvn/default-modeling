{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d77bc3c-145c-45b4-b118-91d8cf782aa1",
   "metadata": {},
   "source": [
    "### BUILD FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e65454-1c00-44e8-8a5d-fc35a2f02893",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/trainer.pyx\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "\n",
    "from ..utils.preproc import CategoricalEncoder\n",
    "from ..utils.preproc import NumericEncoder\n",
    "from ..utils.preproc import feature_definition\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def train():\n",
    "        \n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train-folder\", type=str, default=os.environ.get(\"TRAIN_FOLDER\"))\n",
    "    parser.add_argument(\"--model-name\", type=str, default=os.environ.get(\"MODEL_NAME\"))\n",
    "    parser.add_argument(\"--target\", type=str, default=os.environ.get(\"TARGET\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=os.environ.get(\"TRAIN_FILE\"))\n",
    "    # RF parameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=10)\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=10)  \n",
    "    parser.add_argument(\"--random-state\", type=int, default=1234)   \n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(args)\n",
    "    start_time = time.time()\n",
    "    print(f\"Training Data at {os.path.join(args.train_folder, args.train_file)}\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train_folder, args.train_file))\n",
    "    y_train = train_df[args.target]\n",
    "     \n",
    "    categories_features, numerics_features = feature_definition()\n",
    "    all_features = categories_features + numerics_features + [args.target]\n",
    "    categories_features.append(args.target)\n",
    "    input_features = categories_features + numerics_features\n",
    "    print(\"Total Input Features\", len(input_features))\n",
    "\n",
    "    # Preproc Data\n",
    "    numeric_encoder = NumericEncoder\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('numeric_encoder', numeric_encoder(column_list=numerics_features, \n",
    "                                            bin_width=1))])\n",
    "    \n",
    "    categorical_encoder = CategoricalEncoder\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('categorical_encoder', categorical_encoder(column_list=categories_features))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categories_features),\n",
    "            ('num', numeric_transformer, numerics_features)],\n",
    "        remainder=\"drop\")\n",
    "    \n",
    "    class_weight_list = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                          classes=np.unique(y_train),\n",
    "                                                          y=y_train)\n",
    "    class_weight_dict = {}\n",
    "    for i, weight in enumerate(class_weight_list):\n",
    "        class_weight_dict[i] = weight\n",
    "    print('class weight', class_weight_dict)\n",
    "\n",
    "    rf_model = RandomForestClassifier(\n",
    "                n_estimators=args.n_estimators, \n",
    "                min_samples_leaf=args.min_samples_leaf, \n",
    "                max_depth=args.max_depth, \n",
    "                class_weight=class_weight_dict,\n",
    "                random_state=args.random_state,\n",
    "                n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    ml_pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", rf_model)\n",
    "    ])\n",
    "    \n",
    "    ml_pipeline.fit(train_df[all_features], y_train)\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "    saved_name = f\"{os.path.join(args.model_dir, args.model_name)}.joblib\"\n",
    "    \n",
    "    if os.path.isfile(saved_name):  \n",
    "        print(f\"Found existing model at: {saved_name}.\\nOverwriting ...\")\n",
    "    \n",
    "    joblib.dump(ml_pipeline, saved_name)\n",
    "    print(f\"Congratulation! Saving model at {saved_name}. Finish after {time.time() - start_time} s\")    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3253fd-48a0-4c97-bba2-90a7dbbb28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/predictor.pyx\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import errno\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp \n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "\n",
    "from ..utils.preproc import CategoricalEncoder\n",
    "from ..utils.preproc import NumericEncoder\n",
    "from ..utils.preproc import feature_definition\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns:\n",
    "    Raise: FileNotFoundError if model hasn't been found\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"MODEL_DIR\"))\n",
    "    parser.add_argument(\"--test-folder\", type=str, default=os.environ.get(\"TEST_FOLDER\"))\n",
    "    parser.add_argument(\"--model-name\", type=str, default=os.environ.get(\"MODEL_NAME\"))\n",
    "    parser.add_argument(\"--target\", type=str, default=os.environ.get(\"TARGET\"))\n",
    "    parser.add_argument(\"--test-file\", type=str, default=os.environ.get(\"TEST_FILE\"))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    model_path = os.path.join(args.model_dir, args.model_name)\n",
    "    model_file = f\"{model_path}.joblib\"\n",
    "    print(args)\n",
    "    if not os.path.isfile(model_file):\n",
    "        raise(FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), model_file))\n",
    "        \n",
    "    print(f\"Found model at: {model_file}\")\n",
    "    \n",
    "    risk_model = joblib.load(model_file)\n",
    "    test_df = pd.read_csv(os.path.join(args.test_folder, args.test_file))\n",
    "    categories_features, numerics_features = feature_definition()\n",
    "    all_features = categories_features + numerics_features + [\"default\"]\n",
    "    print(f\"Predicting {args.test_file} ....\")\n",
    "    start_time = time.time()\n",
    "    y_test_pred = risk_model.predict_proba(test_df[all_features])\n",
    "    print(f\"Finish after {time.time() - start_time} s\")\n",
    "    y_test_pred = y_test_pred[:, 1]\n",
    "    test_df[\"default_prediction\"] = y_test_pred\n",
    "    \n",
    "    saved_filed = os.path.join(args.test_folder, f\"{args.test_file}\")\n",
    "    print(f\"...to csv {saved_filed}\")\n",
    "    test_df.to_csv(saved_filed, index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d209fc-2006-48f3-bcca-a5500b813578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/default_modeling/utils/load.pyx\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from typing import Union\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_data(event_data: Union[list, str]) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Takes the data returned from Cassadra queries and converts them into a\n",
    "    DataFrame that can be digested.\n",
    "\n",
    "    Args:\n",
    "      event_data(list[dict] or string): The data returned from sedds Cassandra client fetch method or the name of a csv file\n",
    "    Returns:\n",
    "     pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    if not event_data:\n",
    "        LOGGER.error(\"event_data is empty\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if isinstance(event_data, str) or isinstance(event_data, pathlib.PosixPath):\n",
    "        data = pd.read_csv(event_data)\n",
    "    else:\n",
    "        data = pd.DataFrame(event_data)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98d3ab-1eb8-4914-9de4-39fee6aadabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/default_modeling/utils/preproc.pyx\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from category_encoders.woe import WOEEncoder\n",
    "\n",
    "def feature_definition():\n",
    "    \n",
    "    \"\"\"\n",
    "    Define list of categorical/numerical features\n",
    "    \"\"\"\n",
    "    # categories\n",
    "    categories = ['category_1', 'category_2', 'category_3', 'category_4', 'category_5', 'category_6', 'category_7',\n",
    "                  'category_8', 'category_9', 'category_10', 'category_11', 'category_12', 'category_13', 'category_14',\n",
    "                  'category_15']\n",
    "    # numerics\n",
    "    numerics = ['numeric_0', 'numeric_1', 'numeric_2', 'numeric_3', 'numeric_4', 'numeric_5', 'numeric_6', 'numeric_7',\n",
    "                'numeric_8', 'numeric_9', 'numeric_10', 'numeric_11', 'numeric_12', 'numeric_13', 'numeric_14', \n",
    "                'numeric_15', 'numeric_16', 'numeric_17', 'numeric_18', 'numeric_19', 'numeric_20', 'numeric_21',\n",
    "                'numeric_22']\n",
    "    \n",
    "    return categories, numerics\n",
    "\n",
    "\n",
    "# ----- PREPROC ------\n",
    "class NumericEncoder():\n",
    "    \n",
    "    \"\"\"\n",
    "    Encode number by binning into different ranges\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 column_list: list = None,\n",
    "                 bin_width: int = None):\n",
    "        \n",
    "        self.column_list = column_list\n",
    "        self.bin_width = bin_width\n",
    "        \n",
    "    def __binning__(self, \n",
    "                    X: pd.Series, \n",
    "                    bucket_list: list, \n",
    "                    bin_width: int) -> list:\n",
    "        \"\"\"\n",
    "        Helper function to bin a series\n",
    "        Args:\n",
    "            X: continuous value Series\n",
    "            bucket_list: list of different value for each bin\n",
    "                         Some features require specific binning values\n",
    "            bin_width: auto-bin with width percentage\n",
    "            (Either bin_width or bucket_list is used)\n",
    "        Returns:\n",
    "            list of binned values\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.copy(deep=True)\n",
    "        n_null = X.isna().sum()\n",
    "\n",
    "        if n_null > 0:\n",
    "            X = X.fillna(-1)\n",
    "            bucket_bin = [-1]\n",
    "        else:\n",
    "            bucket_bin = []\n",
    "\n",
    "        if bucket_list is None:\n",
    "            bucket_list = list(range(0, 100 + bin_width, bin_width))\n",
    "            for i, q in enumerate(bucket_list):\n",
    "                q_quantile = round(np.percentile(X.astype(np.float32).values, q), 3)\n",
    "                if q_quantile not in bucket_bin:\n",
    "                    bucket_bin.append(q_quantile)\n",
    "\n",
    "        else:\n",
    "            bucket_bin = bucket_bin + list(bucket_list)\n",
    "            \n",
    "        return bucket_bin\n",
    "    \n",
    "    def fit(self, \n",
    "            X: pd.DataFrame, \n",
    "            y: Union[list, np.array] = None, \n",
    "            verbose: int = 0):\n",
    "        \"\"\"\n",
    "        Construct encoder as a dictionary\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "            y: np.array Output\n",
    "            verbose: int. for logging info\n",
    "        Return:\n",
    "            encoder object\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        encode_dict = {}\n",
    "        for column in self.column_list:\n",
    "            if column != \"age\":\n",
    "                # Encode other columns\n",
    "                encode_dict[column] = self.__binning__(X[column], None, self.bin_width)\n",
    "            elif column == \"age\":\n",
    "                # Specific encoding for age columns\n",
    "                max_age = max(X[column])\n",
    "                age_bucket=[0, 18, 24, 40, 57, 75, max_age]\n",
    "                encode_dict[column] = self.__binning__(X[column], age_bucket, self.bin_width)\n",
    "            if verbose:\n",
    "                print('\\n', column)\n",
    "                print(encode_dict)\n",
    "        self.encoder = encode_dict\n",
    "        return self\n",
    "        \n",
    "    def transform(self, \n",
    "                  X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use built encode to transform data\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "        Return:\n",
    "            pd.DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "\n",
    "        for col in self.column_list:     \n",
    "            # if boolean, convert to int\n",
    "            if X[col].dtype == bool:\n",
    "                X[col] = X[col].astype(int)\n",
    "            \n",
    "            if X[col].isnull().any():\n",
    "                X[col] = X[col].fillna(-1)\n",
    "            \n",
    "            bucket_bin = self.encoder[col]\n",
    "\n",
    "            # Extend bin range if values exceed\n",
    "            if max(bucket_bin) < max(X[col]):\n",
    "                bucket_bin[-1] = max(X[col])\n",
    "            if  min(bucket_bin) > min(X[col]):\n",
    "                bucket_bin[0] = min(X[col])\n",
    "\n",
    "            X[col] = pd.cut(X[col],\n",
    "                            bucket_bin,\n",
    "                            include_lowest=True,\n",
    "                            retbins=True,\n",
    "                            labels=bucket_bin[:-1])[0].astype(float)\n",
    "        return X\n",
    "\n",
    "class CategoricalEncoder():\n",
    "\n",
    "    \"\"\"\n",
    "    Encode categories by Weight of Evidence \n",
    "    (from category_encoders library)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 column_list: list = None):\n",
    "        self.encoder = None\n",
    "        self.column_list = column_list\n",
    "    \n",
    "    def fit(self, \n",
    "            X: pd.DataFrame,\n",
    "            y: Union[list, np.array],\n",
    "            verbose: int = 0):\n",
    "        \"\"\"\n",
    "        Construct encoder as a dictionary\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "            y: np.array Output\n",
    "            verbose: int. for logging info\n",
    "        Return:\n",
    "            encoder object\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        woe_encoder = WOEEncoder(cols=self.column_list, random_state=50)\n",
    "        woe_encoder = woe_encoder.fit(X[self.column_list], y)\n",
    "        self.encoder = woe_encoder\n",
    "        return self\n",
    "                        \n",
    "    def transform(self, \n",
    "                  X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use built encode to transform data\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "        Return:\n",
    "            pd.DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        X[self.column_list] = self.encoder.transform(X[self.column_list])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95926b-cfa2-4279-b86f-05eab9a8d8fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CYTHON INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64075c3-fa2a-455e-aba9-e29783e08ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/launch_trainer.py\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "from .trainer import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076644c-10c7-4bf4-be01-dec31fd2c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/launch_predictor.py\n",
    "import pyximport\n",
    "pyximport.install()\n",
    "from .predictor import predict\n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaf65e-aa6a-4755-b020-c848c7512f49",
   "metadata": {},
   "source": [
    "### CYTHONIZE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcdddb5-33c5-42d3-ba43-7b210c3beb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default_modeling/setup.py\n",
    "from distutils.core import setup\n",
    "import setuptools\n",
    "from Cython.Build import cythonize\n",
    "\n",
    "setup(\n",
    "    name='default_modeling',\n",
    "    version='0.0.1',\n",
    "    description=\"Default Probability Estimation Library\",\n",
    "    author=\"Linh, V. Nguyen\",\n",
    "    author_email=\"linhvietnguyen.ee@gmail.com\",\n",
    "    url=\"https://github.com/nvlinhvn/default-modeling/default_modeling\",\n",
    "    include_package_data=True,\n",
    "    install_requires=[\n",
    "        \"pandas>=1.3.4\",\n",
    "        \"numpy>=1.21.3\",\n",
    "        \"scikit-learn>=1.0.0\",\n",
    "        \"category_encoders>=2.3.0\",\n",
    "        \"Cython>=0.29.21\",\n",
    "        \"scipy>=1.7.0\",\n",
    "    ],\n",
    "    extras_require={\"dev\": [\"joblib\" ]},\n",
    "    ext_modules=cythonize([\"default_modeling/default_modeling/utils/*.pyx\",\n",
    "                           \"default_modeling/default_modeling/interface/*.pyx\"],\n",
    "                          language_level = \"3\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05beb050-7c9a-4d19-8f88-ba43b7454227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m default_modeling.setup build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87636ca-1322-4244-ac44-b15379dd90ce",
   "metadata": {},
   "source": [
    "### LOCAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d14de5-bc89-408d-9bfc-06448fbc9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m unittest discover default_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194f5c4-8b62-4e91-9843-0bd6d6b27d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m default_modeling.default_modeling.interface.launch_trainer \\\n",
    "                                                --model-dir ./model \\\n",
    "                                                --model-name risk_model \\\n",
    "                                                --train-folder train_data \\\n",
    "                                                --train-file train_set_2.csv \\\n",
    "                                                --target default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5837bae-0ec3-447f-9dc2-4eabd8ab2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m default_modeling.default_modeling.interface.launch_predictor --test-file test_set_1.csv \\\n",
    "                                               --model-dir ./model \\\n",
    "                                               --model-name risk_model \\\n",
    "                                               --test-folder test_data \\\n",
    "                                               --test-file test_set_1.csv \\\n",
    "                                               --target default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4890360-2dbd-4eab-89a4-1c76c84b850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.8\n",
    "WORKDIR /app/\n",
    "\n",
    "RUN mkdir model\n",
    "\n",
    "ENV TRAIN_FOLDER=./train_data\n",
    "ENV TEST_FOLDER=./test_data\n",
    "ENV TRAIN_FILE=train_set.csv\n",
    "ENV TEST_FILE=test_set.csv\n",
    "ENV MODEL_DIR=./model\n",
    "ENV MODEL_NAME=risk_model\n",
    "ENV TARGET=default\n",
    "\n",
    "COPY requirements.txt .\n",
    "COPY default_modeling default_modeling\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "RUN python3 -m default_modeling.setup build\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e6790-cee9-490f-8095-36b843d79c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build --no-cache -t default_model -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e60fb-d75d-4d7f-b970-3248e255bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -t default_model:latest -m unittest discover default_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7155ce-1703-42aa-9cd7-38cc3e8451b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -v /home/jupyter/Cython/train_data:/app/train_data \\\n",
    "            -v /home/jupyter/Cython/model:/app/model \\\n",
    "            default_model:latest -m default_modeling.default_modeling.interface.launch_trainer \\\n",
    "            --train-file train_set_1.csv \\\n",
    "            --n-estimators 200 \\\n",
    "            --max-depth 15 \\\n",
    "            --min-samples-leaf 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ef7b2-3ec3-45e1-81f0-ef9d25ecb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -v /home/jupyter/Cython/test_data:/app/test_data \\\n",
    "            -v /home/jupyter/Cython/model:/app/model \\\n",
    "            default_model:latest -m default_modeling.default_modeling.interface.launch_predictor \\\n",
    "            --test-file test_set_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73765424-8ce2-4ce7-ab4a-9043c0b06803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
