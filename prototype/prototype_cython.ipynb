{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d77bc3c-145c-45b4-b118-91d8cf782aa1",
   "metadata": {},
   "source": [
    "### BUILD FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e65454-1c00-44e8-8a5d-fc35a2f02893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/default_modeling/interface/trainer.pyx\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/trainer.pyx\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from ..utils.preproc import CategoricalEncoder\n",
    "from ..utils.preproc import NumericEncoder\n",
    "from ..utils.preproc import feature_definition\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def train():\n",
    "        \n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train-folder\", type=str, default=os.environ.get(\"TRAIN_FOLDER\"))\n",
    "    parser.add_argument(\"--model-name\", type=str, default=os.environ.get(\"MODEL_NAME\"))\n",
    "    parser.add_argument(\"--target\", type=str, default=os.environ.get(\"TARGET\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=os.environ.get(\"TRAIN_FILE\"))\n",
    "    # RF parameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=10)\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=10)  \n",
    "    parser.add_argument(\"--random-state\", type=int, default=1234)   \n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(args)\n",
    "    start_time = time.time()\n",
    "    print(f\"Training Data at {os.path.join(args.train_folder, args.train_file)}\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train_folder, args.train_file))\n",
    "    y_train = train_df[args.target]\n",
    "     \n",
    "    categories_features, numerics_features = feature_definition()\n",
    "    all_features = categories_features + numerics_features + [args.target]\n",
    "    categories_features.append(args.target)\n",
    "    input_features = categories_features + numerics_features\n",
    "    print(\"Total Input Features\", len(input_features))\n",
    "\n",
    "    # Preproc Data\n",
    "    numeric_encoder = NumericEncoder\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('numeric_encoder', numeric_encoder(column_list=numerics_features, \n",
    "                                            bin_width=1))])\n",
    "    \n",
    "    categorical_encoder = CategoricalEncoder\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('categorical_encoder', categorical_encoder(column_list=categories_features))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categories_features),\n",
    "            ('num', numeric_transformer, numerics_features)],\n",
    "        remainder=\"drop\")\n",
    "    \n",
    "    class_weight_list = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                          classes=np.unique(y_train),\n",
    "                                                          y=y_train)\n",
    "    class_weight_dict = {}\n",
    "    for i, weight in enumerate(class_weight_list):\n",
    "        class_weight_dict[i] = weight\n",
    "    print('class weight', class_weight_dict)\n",
    "\n",
    "    rf_model = RandomForestClassifier(\n",
    "                n_estimators=args.n_estimators, \n",
    "                min_samples_leaf=args.min_samples_leaf, \n",
    "                max_depth=args.max_depth, \n",
    "                class_weight=class_weight_dict,\n",
    "                random_state=args.random_state,\n",
    "                n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    ml_pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", rf_model)\n",
    "    ])\n",
    "    \n",
    "    ml_pipeline.fit(train_df[all_features], y_train)\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "    saved_name = f\"{os.path.join(args.model_dir, args.model_name)}.joblib\"\n",
    "    \n",
    "    if os.path.isfile(saved_name):  \n",
    "        print(f\"Found existing model at: {saved_name}.\\nOverwriting ...\")\n",
    "    \n",
    "    joblib.dump(ml_pipeline, saved_name)\n",
    "    print(f\"Congratulation! Saving model at {saved_name}. Finish after {time.time() - start_time} s\")    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3253fd-48a0-4c97-bba2-90a7dbbb28a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/default_modeling/interface/predictor.pyx\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/predictor.pyx\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp \n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from ..utils.preproc import CategoricalEncoder\n",
    "from ..utils.preproc import NumericEncoder\n",
    "from ..utils.preproc import feature_definition\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import errno\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp \n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from ..utils.preproc import CategoricalEncoder\n",
    "from ..utils.preproc import NumericEncoder\n",
    "from ..utils.preproc import feature_definition\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns:\n",
    "    Raise: FileNotFoundError if model hasn't been found\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"MODEL_DIR\"))\n",
    "    parser.add_argument(\"--test-folder\", type=str, default=os.environ.get(\"TEST_FOLDER\"))\n",
    "    parser.add_argument(\"--model-name\", type=str, default=os.environ.get(\"MODEL_NAME\"))\n",
    "    parser.add_argument(\"--target\", type=str, default=os.environ.get(\"TARGET\"))\n",
    "    parser.add_argument(\"--test-file\", type=str, default=os.environ.get(\"TEST_FILE\"))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    model_path = os.path.join(args.model_dir, args.model_name)\n",
    "    model_file = f\"{model_path}.joblib\"\n",
    "    print(args)\n",
    "    if not os.path.isfile(model_file):\n",
    "        raise(FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), model_file))\n",
    "        \n",
    "    print(f\"Found model at: {model_file}\")\n",
    "    \n",
    "    risk_model = joblib.load(model_file)\n",
    "    test_df = pd.read_csv(os.path.join(args.test_folder, args.test_file))\n",
    "    categories_features, numerics_features = feature_definition()\n",
    "    all_features = categories_features + numerics_features + [\"default\"]\n",
    "    print(f\"Predicting {args.test_file} ....\")\n",
    "    start_time = time.time()\n",
    "    y_test_pred = risk_model.predict_proba(test_df[all_features])\n",
    "    print(f\"Finish after {time.time() - start_time} s\")\n",
    "    y_test_pred = y_test_pred[:, 1]\n",
    "    test_df[\"default_prediction\"] = y_test_pred\n",
    "    \n",
    "    saved_filed = os.path.join(args.test_folder, f\"{args.test_file}\")\n",
    "    print(f\"...to csv {saved_filed}\")\n",
    "    test_df.to_csv(saved_filed, index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95926b-cfa2-4279-b86f-05eab9a8d8fb",
   "metadata": {},
   "source": [
    "### CYTHON INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64075c3-fa2a-455e-aba9-e29783e08ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/default_modeling/interface/launch_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/launch_training.py\n",
    "from .trainer import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0076644c-10c7-4bf4-be01-dec31fd2c46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/default_modeling/interface/launch_predicting.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/default_modeling/interface/launch_predicting.py\n",
    "from .predictor import predict\n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaf65e-aa6a-4755-b020-c848c7512f49",
   "metadata": {},
   "source": [
    "### CYTHONIZE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdddb5-33c5-42d3-ba43-7b210c3beb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/setup.py\n",
    "from Cython.Build import cythonize\n",
    "from distutils.core import setup\n",
    "\n",
    "setup(\n",
    "    name='default_modeling',\n",
    "    version='0.0.1',\n",
    "    description=\"Default Probability Estimation Library\",\n",
    "    author=\"Linh, V. Nguyen\",\n",
    "    author_email=\"linhvietnguyen.ee@gmail.com\",\n",
    "    ext_modules=cythonize([\"default_modeling/default_modeling/interface/*.pyx\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219b30a3-c3ef-449b-8c80-23d793fece40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling default_modeling/default_modeling/interface/predictor.pyx because it changed.\n",
      "Compiling default_modeling/default_modeling/interface/trainer.pyx because it changed.\n",
      "[1/2] Cythonizing default_modeling/default_modeling/interface/predictor.pyx\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/jupyter/Cython/default_modeling/default_modeling/interface/predictor.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "[2/2] Cythonizing default_modeling/default_modeling/interface/trainer.pyx\n",
      "/opt/conda/lib/python3.7/site-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /home/jupyter/Cython/default_modeling/default_modeling/interface/trainer.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "running build_ext\n",
      "building 'default_modeling.default_modeling.interface.predictor' extension\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.7m -c default_modeling/default_modeling/interface/predictor.c -o build/temp.linux-x86_64-3.7/default_modeling/default_modeling/interface/predictor.o\n",
      "gcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/default_modeling/default_modeling/interface/predictor.o -o /home/jupyter/Cython/default_modeling/default_modeling/interface/predictor.cpython-37m-x86_64-linux-gnu.so\n",
      "building 'default_modeling.default_modeling.interface.trainer' extension\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/include/python3.7m -c default_modeling/default_modeling/interface/trainer.c -o build/temp.linux-x86_64-3.7/default_modeling/default_modeling/interface/trainer.o\n",
      "gcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/default_modeling/default_modeling/interface/trainer.o -o /home/jupyter/Cython/default_modeling/default_modeling/interface/trainer.cpython-37m-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "!python3 -m default_modeling.setup build_ext --inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87636ca-1322-4244-ac44-b15379dd90ce",
   "metadata": {},
   "source": [
    "### LOCAL RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4194f5c4-8b62-4e91-9843-0bd6d6b27d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Namespace(max_depth=10, min_samples_leaf=10, model_dir='./model', model_name='risk_model', n_estimators=100, random_state=1234, target='default', train_file='train_set_1.csv', train_folder='train_data')\n",
      "Training Data at train_data/train_set_1.csv\n",
      "('Total Input Features', 39)\n",
      "('class weight', {0: 0.5071993428787708, 1: 35.22539149888143})\n",
      "Congratulation! Saving model at ./model/risk_model.joblib. Finish after 4.053596019744873 s\n"
     ]
    }
   ],
   "source": [
    "!python3 -m default_modeling.default_modeling.interface.launch_training --train-file train_set_1.csv \\\n",
    "                                                --model-dir ./model \\\n",
    "                                                --model-name risk_model \\\n",
    "                                                --train-folder train_data \\\n",
    "                                                --train-file train_set_1.csv \\\n",
    "                                                --target default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5837bae-0ec3-447f-9dc2-4eabd8ab2faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Namespace(model_dir='./model', model_name='risk_model', target='default', test_file='test_set_1.csv', test_folder='test_data')\n",
      "Found model at: ./model/risk_model.joblib\n",
      "Predicting test_set_1.csv ....\n",
      "Finish after 0.44262242317199707 s\n",
      "...to csv test_data/test_set_1.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 -m default_modeling.default_modeling.interface.launch_predicting --test-file test_set_1.csv \\\n",
    "                                               --model-dir ./model \\\n",
    "                                               --model-name risk_model \\\n",
    "                                               --test-folder test_data \\\n",
    "                                               --test-file test_set_1.csv \\\n",
    "                                               --target default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4890360-2dbd-4eab-89a4-1c76c84b850a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.8\n",
    "WORKDIR /app/\n",
    "\n",
    "RUN mkdir model\n",
    "\n",
    "ENV TRAIN_FOLDER=./train_data\n",
    "ENV TEST_FOLDER=./test_data\n",
    "ENV TRAIN_FILE=train_set.csv\n",
    "ENV TEST_FILE=test_set.csv\n",
    "ENV MODEL_DIR=./model\n",
    "ENV MODEL_NAME=risk_model\n",
    "ENV TARGET=default\n",
    "\n",
    "COPY requirements.txt .\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "COPY default_modeling default_modeling\n",
    "RUN python3 -m default_modeling.setup build_ext --inplace\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11e6790-cee9-490f-8095-36b843d79c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  46.24MB\n",
      "Step 1/15 : FROM python:3.8\n",
      " ---> 79372a158581\n",
      "Step 2/15 : WORKDIR /app/\n",
      " ---> Running in 76046c31486d\n",
      "Removing intermediate container 76046c31486d\n",
      " ---> 74791ab2cde2\n",
      "Step 3/15 : RUN mkdir model\n",
      " ---> Running in 48706c67d0be\n",
      "Removing intermediate container 48706c67d0be\n",
      " ---> dbcf3cc43d04\n",
      "Step 4/15 : ENV TRAIN_FOLDER=./train_data\n",
      " ---> Running in 8e4af9db8dbc\n",
      "Removing intermediate container 8e4af9db8dbc\n",
      " ---> 4e42ae91a8b4\n",
      "Step 5/15 : ENV TEST_FOLDER=./test_data\n",
      " ---> Running in 2be3a2d3fd67\n",
      "Removing intermediate container 2be3a2d3fd67\n",
      " ---> 6556e99e1922\n",
      "Step 6/15 : ENV TRAIN_FILE=train_set.csv\n",
      " ---> Running in cff90288c8c0\n",
      "Removing intermediate container cff90288c8c0\n",
      " ---> 0cd143d353d6\n",
      "Step 7/15 : ENV TEST_FILE=test_set.csv\n",
      " ---> Running in f4dd9031918a\n",
      "Removing intermediate container f4dd9031918a\n",
      " ---> 8146cac52b76\n",
      "Step 8/15 : ENV MODEL_DIR=./model\n",
      " ---> Running in 13117e5d5084\n",
      "Removing intermediate container 13117e5d5084\n",
      " ---> fa28f81e2778\n",
      "Step 9/15 : ENV MODEL_NAME=risk_model\n",
      " ---> Running in f4728c314c2b\n",
      "Removing intermediate container f4728c314c2b\n",
      " ---> 604da920aca2\n",
      "Step 10/15 : ENV TARGET=default\n",
      " ---> Running in 32a3184ba283\n",
      "Removing intermediate container 32a3184ba283\n",
      " ---> dd86658e8c41\n",
      "Step 11/15 : COPY requirements.txt .\n",
      " ---> b379ea4cb75c\n",
      "Step 12/15 : RUN pip install -r requirements.txt\n",
      " ---> Running in a20de488f5b1\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.24-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Downloading scikit_learn-1.0.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "Collecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n",
      "Collecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "Collecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=a2491dc959d818e18191f7a268dff6b09ed16d49dc3f046e77fedc8d5357afaa\n",
      "  Stored in directory: /root/.cache/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: six, pytz, python-dateutil, numpy, threadpoolctl, scipy, patsy, pandas, joblib, statsmodels, scikit-learn, sklearn, Cython, category-encoders\n",
      "Successfully installed Cython-0.29.24 category-encoders-2.3.0 joblib-1.1.0 numpy-1.21.3 pandas-1.3.4 patsy-0.5.2 python-dateutil-2.8.2 pytz-2021.3 scikit-learn-1.0.1 scipy-1.7.1 six-1.16.0 sklearn-0.0 statsmodels-0.13.0 threadpoolctl-3.0.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container a20de488f5b1\n",
      " ---> de4b417f41d2\n",
      "Step 13/15 : COPY default_modeling default_modeling\n",
      " ---> 6e4a3a19531d\n",
      "Step 14/15 : RUN python3 -m default_modeling.setup build_ext --inplace\n",
      " ---> Running in f20042e3a6c3\n",
      "running build_ext\n",
      "building 'default_modeling.default_modeling.interface.predictor' extension\n",
      "creating build\n",
      "creating build/temp.linux-x86_64-3.8\n",
      "creating build/temp.linux-x86_64-3.8/default_modeling\n",
      "creating build/temp.linux-x86_64-3.8/default_modeling/default_modeling\n",
      "creating build/temp.linux-x86_64-3.8/default_modeling/default_modeling/interface\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.8 -c default_modeling/default_modeling/interface/predictor.c -o build/temp.linux-x86_64-3.8/default_modeling/default_modeling/interface/predictor.o\n",
      "gcc -pthread -shared build/temp.linux-x86_64-3.8/default_modeling/default_modeling/interface/predictor.o -L/usr/local/lib -o /app/default_modeling/default_modeling/interface/predictor.cpython-38-x86_64-linux-gnu.so\n",
      "building 'default_modeling.default_modeling.interface.trainer' extension\n",
      "gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.8 -c default_modeling/default_modeling/interface/trainer.c -o build/temp.linux-x86_64-3.8/default_modeling/default_modeling/interface/trainer.o\n",
      "gcc -pthread -shared build/temp.linux-x86_64-3.8/default_modeling/default_modeling/interface/trainer.o -L/usr/local/lib -o /app/default_modeling/default_modeling/interface/trainer.cpython-38-x86_64-linux-gnu.so\n",
      "Removing intermediate container f20042e3a6c3\n",
      " ---> ed01cfb0726d\n",
      "Step 15/15 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in 206bd75bf112\n",
      "Removing intermediate container 206bd75bf112\n",
      " ---> 4e4a84cfeb46\n",
      "Successfully built 4e4a84cfeb46\n",
      "Successfully tagged default_model:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build --no-cache -t default_model -f Dockerfile ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "978e60fb-d75d-4d7f-b970-3248e255bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following test data\n",
      "default_modeling/tests/data/test_sample_1.csv\n",
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.640s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!docker run -t default_model:latest -m unittest discover default_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7155ce-1703-42aa-9cd7-38cc3e8451b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Namespace(max_depth=10, min_samples_leaf=10, model_dir='./model', model_name='risk_model', n_estimators=100, random_state=1234, target='default', train_file='train_set_1.csv', train_folder='./train_data')\n",
      "Training Data at ./train_data/train_set_1.csv\n",
      "('Total Input Features', 39)\n",
      "('class weight', {0: 0.5071993428787708, 1: 35.22539149888143})\n",
      "Found existing model at: ./model/risk_model.joblib.\n",
      "Overwriting ...\n",
      "Congratulation! Saving model at ./model/risk_model.joblib. Finish after 4.340307712554932 s\n"
     ]
    }
   ],
   "source": [
    "!docker run -v /home/jupyter/Cython/train_data:/app/train_data \\\n",
    "            -v /home/jupyter/Cython/model:/app/model \\\n",
    "            default_model:latest -m default_modeling.default_modeling.interface.launch_training \\\n",
    "            --train-file train_set_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc9ef7b2-3ec3-45e1-81f0-ef9d25ecb563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Namespace(model_dir='./model', model_name='risk_model', target='default', test_file='test_set_1.csv', test_folder='./test_data')\n",
      "Found model at: ./model/risk_model.joblib\n",
      "Predicting test_set_1.csv ....\n",
      "Finish after 0.40616917610168457 s\n",
      "...to csv ./test_data/test_set_1.csv\n"
     ]
    }
   ],
   "source": [
    "!docker run -v /home/jupyter/Cython/test_data:/app/test_data \\\n",
    "            -v /home/jupyter/Cython/model:/app/model \\\n",
    "            default_model:latest -m default_modeling.default_modeling.interface.launch_predicting \\\n",
    "            --test-file test_set_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621832ee-9051-4d13-8799-886f374bd3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
