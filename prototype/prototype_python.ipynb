{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52331e9-5507-4aee-ba10-9250c9012f04",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2991c2cc-2544-40f0-bdff-bf599bb16264",
   "metadata": {},
   "source": [
    "#### PREPROC FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "174eef4a-e090-4220-bc05-048d36b81ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/default_modeling/utils/preproc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/default_modeling/utils/preproc.py\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from category_encoders.woe import WOEEncoder\n",
    "\n",
    "def feature_definition():\n",
    "    \n",
    "    numerics = ['age', 'account_amount_added_12_24m', 'account_days_in_rem_12_24m', 'account_days_in_term_12_24m',\n",
    "                'account_incoming_debt_vs_paid_0_24m', 'avg_payment_span_0_12m', 'avg_payment_span_0_3m',\n",
    "                'max_paid_inv_0_12m', 'max_paid_inv_0_24m', 'num_active_div_by_paid_inv_0_12m',\n",
    "                'num_active_inv', 'num_arch_dc_0_12m', 'num_arch_dc_12_24m', 'num_arch_ok_0_12m', \n",
    "                'num_arch_ok_12_24m', 'num_arch_rem_0_12m', 'num_arch_written_off_12_24m',\n",
    "                'num_unpaid_bills', 'sum_capital_paid_account_0_12m', 'sum_capital_paid_account_12_24m',\n",
    "                'has_paid',\n",
    "                'sum_paid_inv_0_12m', 'time_hours']\n",
    "    categories = ['account_status', 'account_worst_status_0_3m', 'account_worst_status_12_24m',\n",
    "                  'account_worst_status_3_6m', 'account_worst_status_6_12m', 'status_last_archived_0_24m',\n",
    "                  'status_2nd_last_archived_0_24m', 'status_3rd_last_archived_0_24m', 'status_max_archived_0_6_months',\n",
    "                  'status_max_archived_0_12_months', 'status_max_archived_0_24_months',\n",
    "                  'worst_status_active_inv', 'merchant_category', 'merchant_group', 'name_in_email']\n",
    "    \n",
    "    return categories, numerics\n",
    "\n",
    "\n",
    "# ----- PREPROC ------\n",
    "class NumericEncoder():\n",
    "    \n",
    "    \"\"\"\n",
    "    Encode number by binning into different ranges\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 column_list: list = None,\n",
    "                 bin_width: int = None):\n",
    "        \n",
    "        self.column_list = column_list\n",
    "        self.bin_width = bin_width\n",
    "        \n",
    "    def __binning__(self, \n",
    "                    X: pd.Series, \n",
    "                    bucket_list: list, \n",
    "                    bin_width: int) -> list:\n",
    "        \"\"\"\n",
    "        Helper function to bin a series\n",
    "        Args:\n",
    "            X: continuous value Series\n",
    "            bucket_list: list of different value for each bin\n",
    "                         Some features require specific binning values\n",
    "            bin_width: auto-bin with width percentage\n",
    "            (Either bin_width or bucket_list is used)\n",
    "        Returns:\n",
    "            list of binned values\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.copy(deep=True)\n",
    "        n_null = X.isna().sum()\n",
    "\n",
    "        if n_null > 0:\n",
    "            X = X.fillna(-1)\n",
    "            bucket_bin = [-1]\n",
    "        else:\n",
    "            bucket_bin = []\n",
    "\n",
    "        if bucket_list is None:\n",
    "            bucket_list = range(0, 100 + bin_width, bin_width)\n",
    "            for i, q in enumerate(bucket_list):\n",
    "                q_quantile = round(np.percentile(X.astype(np.float32).values, q), 3)\n",
    "                if q_quantile not in bucket_bin:\n",
    "                    bucket_bin.append(q_quantile)\n",
    "\n",
    "        else:\n",
    "            bucket_bin = bucket_bin + list(bucket_list)\n",
    "            \n",
    "        return bucket_bin\n",
    "    \n",
    "    def fit(self, \n",
    "            X: pd.DataFrame, \n",
    "            y: Union[list, np.array] = None, \n",
    "            verbose: int = 0):\n",
    "        \"\"\"\n",
    "        Construct encoder as a dictionary\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "            y: np.array Output\n",
    "            verbose: int. for logging info\n",
    "        Return:\n",
    "            encoder object\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        encode_dict = {}\n",
    "        for column in self.column_list:\n",
    "            if column != \"age\":\n",
    "                # Encode other columns\n",
    "                encode_dict[column] = self.__binning__(X[column], None, self.bin_width)\n",
    "            elif column == \"age\":\n",
    "                # Specific encoding for age columns\n",
    "                max_age = max(X[column])\n",
    "                age_bucket=[0, 18, 24, 40, 57, 75, max_age]\n",
    "                encode_dict[column] = self.__binning__(X[column], age_bucket, self.bin_width)\n",
    "            if verbose:\n",
    "                print('\\n', column)\n",
    "                print(encode_dict)\n",
    "        self.encoder = encode_dict\n",
    "        return self\n",
    "        \n",
    "    def transform(self, \n",
    "                  X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use built encode to transform data\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "        Return:\n",
    "            pd.DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        if \"has_paid\" in X:\n",
    "            # has_paid is boolean\n",
    "            X[\"has_paid\"] = X[\"has_paid\"].astype(int)\n",
    "\n",
    "        for col in self.column_list:        \n",
    "            if X[col].isnull().any():\n",
    "                X[col] = X[col].fillna(-1)\n",
    "            \n",
    "            bucket_bin = self.encoder[col]\n",
    "\n",
    "            # Extend bin range if values exceed\n",
    "            if max(bucket_bin) < max(X[col]):\n",
    "                bucket_bin[-1] = max(X[col])\n",
    "            if  min(bucket_bin) > min(X[col]):\n",
    "                bucket_bin[0] = min(X[col])\n",
    "\n",
    "            X[col] = pd.cut(X[col],\n",
    "                            bucket_bin,\n",
    "                            include_lowest=True,\n",
    "                            retbins=True,\n",
    "                            labels=bucket_bin[:-1])[0].astype(float)\n",
    "        return X\n",
    "\n",
    "class CategoricalEncoder():\n",
    "\n",
    "    \"\"\"\n",
    "    Encode categories by Weight of Evidence \n",
    "    (from category_encoders library)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 column_list: list = None):\n",
    "        self.encoder = None\n",
    "        self.column_list = column_list\n",
    "    \n",
    "    def fit(self, \n",
    "            X: pd.DataFrame,\n",
    "            y: Union[list, np.array],\n",
    "            verbose: int = 0):\n",
    "        \"\"\"\n",
    "        Construct encoder as a dictionary\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "            y: np.array Output\n",
    "            verbose: int. for logging info\n",
    "        Return:\n",
    "            encoder object\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        woe_encoder = WOEEncoder(cols=self.column_list, random_state=50)\n",
    "        woe_encoder = woe_encoder.fit(X[self.column_list], y)\n",
    "        self.encoder = woe_encoder\n",
    "        return self\n",
    "                        \n",
    "    def transform(self, \n",
    "                  X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Use built encode to transform data\n",
    "        Args:\n",
    "            X: pd.DataFrame\n",
    "        Return:\n",
    "            pd.DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        X = X.copy(deep=True)\n",
    "        X[self.column_list] = self.encoder.transform(X[self.column_list])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669b67e-88b8-4dec-b6f7-1b1b660f2e14",
   "metadata": {},
   "source": [
    "#### LOAD FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9b5ef62-227d-42e1-a096-e3ba267cdeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/default_modeling/utils/load.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/default_modeling/utils/load.py \n",
    "import logging\n",
    "import re\n",
    "\n",
    "from typing import Union\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_data(event_data: Union[list, str]) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"Takes the data returned from Cassadra queries and converts them into a\n",
    "    DataFrame that can be digested.\n",
    "\n",
    "    Args:\n",
    "      event_data(list[dict] or string): The data returned from sedds Cassandra client fetch method or the name of a csv file\n",
    "    Returns:\n",
    "     pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    if not event_data:\n",
    "        LOGGER.error(\"event_data is empty\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if isinstance(event_data, str) or isinstance(event_data, pathlib.PosixPath):\n",
    "        data = pd.read_csv(event_data)\n",
    "    else:\n",
    "        data = pd.DataFrame(event_data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be364c79-26f9-4c20-b60f-0d5cf8ef3f40",
   "metadata": {},
   "source": [
    "# UNIT TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fdfb13b-4a76-4520-8c2a-d824f3fc7a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/tests/test_case_base.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/tests/test_case_base.py \n",
    "\"\"\"Base class for unit tests\"\"\"\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import unittest\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from default_modeling.utils.load import load_data\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TestWithData(unittest.TestCase):\n",
    "    raw = dict()\n",
    "    available_file = None\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls) -> None:\n",
    "        cls.available_file = cls.get_available_file()\n",
    "\n",
    "    @classmethod\n",
    "    def get_available_file(cls) -> list:\n",
    "        \"\"\"Returns the list available test data\n",
    "        e.g. everything stored under tests/data\n",
    "        Args:\n",
    "        Returns: List file\n",
    "        \"\"\"\n",
    "        p = pathlib.Path(\".\")\n",
    "        print(\"folder view\")\n",
    "        print([x for x in p.iterdir() if x.is_dir()])\n",
    "        test_data = pathlib.Path(\"default_modeling/tests/data/\").glob(\"*.csv\")\n",
    "        test_data = [f for f in test_data]\n",
    "\n",
    "        print(\"Found the following test data\")\n",
    "        for f in test_data:\n",
    "            print(f)\n",
    "\n",
    "        return test_data\n",
    "\n",
    "    @classmethod\n",
    "    def get_raw(cls, file: str) -> pd.DataFrame:\n",
    "        \"\"\"Lazy loading for raw data; will return a copy of the df\n",
    "\n",
    "        Args:\n",
    "          file: file name of sample test\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if file in cls.raw:\n",
    "            LOGGER.info(\"Found raw data for %s\", file)\n",
    "            return cls.raw[file].copy()\n",
    "        \n",
    "        df = load_data(file)\n",
    "\n",
    "        LOGGER.info(\"Adding raw data for %s\", file)\n",
    "        cls.raw[file] = df\n",
    "\n",
    "        return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db20f2e7-2cac-4258-b179-f0cdc5edf539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/tests/test_data_handling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/tests/test_data_handling.py \n",
    "\n",
    "import pandas.api.types as ptypes\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "\n",
    "from tests.test_case_base import TestWithData\n",
    "from default_modeling.utils.preproc import feature_definition\n",
    "from default_modeling.utils.preproc import NumericEncoder\n",
    "from default_modeling.utils.preproc import CategoricalEncoder\n",
    "\n",
    "\n",
    "class DataHandlingTests(TestWithData):\n",
    "\n",
    "    def test_load_data(self):\n",
    "        \"\"\"Validate that the most important columns are returned by loading function\"\"\"\n",
    "        numerics, categories = feature_definition()\n",
    "        key_column = numerics + categories + [\"default\"]\n",
    "        for file in self.available_file:\n",
    "            df = self.get_raw(file)\n",
    "            for column in key_column:\n",
    "                self.assertIn(column, df)\n",
    "\n",
    "    def test_preproc_function(self):\n",
    "        \"\"\"Validate if the encoding return reasonable values:\n",
    "        Categorical encoding: Only encode categorical columns\n",
    "        Numeric encoding: Only encode numerical columns\n",
    "        Expected result: all columns are numeric        \n",
    "        \"\"\"\n",
    "        categories, numerics  = feature_definition()\n",
    "        numeric_encoder = NumericEncoder(column_list=numerics, \n",
    "                                         bin_width=1)\n",
    "        categorical_encoder = CategoricalEncoder(column_list=categories)\n",
    "        \n",
    "        input_column = numerics + categories\n",
    "\n",
    "        for file in self.available_file:\n",
    "            df = self.get_raw(file)\n",
    "            y = df[\"default\"].values\n",
    "            categorical_encoder.fit(df, y)\n",
    "            cat_transform = categorical_encoder.transform(df)\n",
    "            # all transformed categories must be numerics\n",
    "            assert all(ptypes.is_numeric_dtype(cat_transform[col]) for col in categories)\n",
    "            # all numerical columns must be the same\n",
    "            assert_frame_equal(cat_transform[numerics], df[numerics])\n",
    "            \n",
    "            numeric_encoder.fit(df.copy(), y)\n",
    "            num_transfom = numeric_encoder.transform(df)\n",
    "            # all transformed numerics must be numerics\n",
    "            assert all(ptypes.is_numeric_dtype(num_transfom[col]) for col in numerics)\n",
    "            # all categorical columns must be the same\n",
    "            assert_frame_equal(num_transfom[categories], df[categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fe9d1cf-32c3-45bd-bf9b-3aa8dbf890fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder view\n",
      "[PosixPath('src'), PosixPath('.cache'), PosixPath('.ipython'), PosixPath('.jupyter'), PosixPath('.ipynb_checkpoints'), PosixPath('default_modeling'), PosixPath('.config'), PosixPath('.local'), PosixPath('.docker'), PosixPath('tutorials')]\n",
      "Found the following test data\n",
      "default_modeling/tests/data/test_sample_1.csv\n",
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.681s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python3 -m unittest discover default_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a60b5-75b7-44d4-a0b6-ad502b644fb2",
   "metadata": {},
   "source": [
    "# INTERFACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa485d8e-0d9a-4a45-929c-231f843a4de9",
   "metadata": {},
   "source": [
    "#### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61035710-3c0e-4369-9d04-3ecfce09b4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/train.py\n",
    "#default_modeling/default_modeling/interface/train.py\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from .default_modeling.utils.preproc import CategoricalEncoder\n",
    "from .default_modeling.utils.preproc import NumericEncoder\n",
    "from .default_modeling.utils.preproc import feature_definition\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def train():\n",
    "        \n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"MODEL_DIR\"))\n",
    "    parser.add_argument(\"--datafolder\", type=str, default=\"./train_data/\")\n",
    "    parser.add_argument(\"--model-name\", type=str, default=os.environ.get(\"MODEL_NAME\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train_set.csv\")\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=10)\n",
    "    parser.add_argument(\"--max-depth\", type=int, default=10)  \n",
    "    parser.add_argument(\"--random-state\", type=int, default=1234)   \n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str, default='default'\n",
    "    )\n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"Training Data Preparation\")\n",
    "    print(os.path.join(args.datafolder, args.train_file))\n",
    "    train_df = pd.read_csv(os.path.join(args.datafolder, args.train_file))\n",
    "    y_train = train_df[args.target]\n",
    "     \n",
    "    print(args)\n",
    "    categories_features, numerics_features = feature_definition()\n",
    "    all_features = categories_features + numerics_features + [args.target]\n",
    "    categories_features.append(args.target)\n",
    "    input_features = categories_features + numerics_features\n",
    "    print(\"Total Input Features\", len(input_features))\n",
    "\n",
    "    # Preproc Data\n",
    "    numeric_encoder = NumericEncoder\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('numeric_encoder', numeric_encoder(column_list=numerics_features, \n",
    "                                            bin_width=1))])\n",
    "    \n",
    "    categorical_encoder = CategoricalEncoder\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('categorical_encoder', categorical_encoder(column_list=categories_features))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categories_features),\n",
    "            ('num', numeric_transformer, numerics_features)],\n",
    "        remainder=\"drop\")\n",
    "    \n",
    "    class_weight_list = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                          classes=np.unique(y_train),\n",
    "                                                          y=y_train)\n",
    "    class_weight_dict = {}\n",
    "    for i, weight in enumerate(class_weight_list):\n",
    "        class_weight_dict[i] = weight\n",
    "    print('class weight', class_weight_dict)\n",
    "\n",
    "    rf_model = RandomForestClassifier(\n",
    "                n_estimators=args.n_estimators, \n",
    "                min_samples_leaf=args.min_samples_leaf, \n",
    "                max_depth=args.max_depth, \n",
    "                class_weight=class_weight_dict,\n",
    "                random_state=args.random_state,\n",
    "                n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    ml_pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", rf_model)\n",
    "    ])\n",
    "    \n",
    "    ml_pipeline.fit(train_df[all_features], y_train)\n",
    "    print(\"Saving Pipeline ........\")\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "    joblib.dump(ml_pipeline, os.path.join(args.model_dir, args.model_name))\n",
    "    print(\"Congratulation! Finish.\")    \n",
    "         \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51148415-0e4b-4292-be35-332800835dac",
   "metadata": {},
   "source": [
    "##### TERMINAL SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99d49501-74bf-4fdb-9dbb-71aadf98cc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Training Data Preparation\n",
      "./train_data/train_set.csv\n",
      "Namespace(datafolder='./train_data', max_depth=10, min_samples_leaf=10, model_dir='./default_modeling/default_modeling/interface/', model_name='risk_model.joblib', n_estimators=100, random_state=1234, target='default', train_file='train_set.csv')\n",
      "Total Input Features 39\n",
      "class weight {0: 0.5071993428787708, 1: 35.22539149888143}\n",
      "Saving Pipeline ........\n",
      "Congratulation! Finish.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m default_modeling.train --datafolder ./train_data \\\n",
    "                                   --model-dir ./default_modeling/default_modeling/interface/ \\\n",
    "                                   --model-name risk_model.joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7bfb4acc-4ec7-4aac-b3ec-517801c640bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 default_modeling/default_modeling/interface/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01e7ea94-8955-4253-bd70-ddf21f0dcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m default_modeling.default_modeling.interface.train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80456ab4-658a-494c-b801-7b57eed28d1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8bb34f38-a020-4ac9-838a-4cf2fa9d5dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/predict.py\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp \n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from .default_modeling.utils.preproc import CategoricalEncoder\n",
    "from .default_modeling.utils.preproc import NumericEncoder\n",
    "from .default_modeling.utils.preproc import feature_definition\n",
    "\n",
    "def predict():\n",
    "    \n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"MODEL_DIR\"))\n",
    "    parser.add_argument(\"--datafolder\", type=str, default=os.environ.get(\"TESTING_FOLDER\"))\n",
    "    parser.add_argument(\"--model-name\", type=str, default=os.environ.get(\"MODEL_NAME\"))\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test_set.csv\")\n",
    " \n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str, default=\"default\"\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(f\"Model path: {os.path.join(args.model_dir, args.model_name)}\")\n",
    "    risk_model = joblib.load(os.path.join(args.model_dir, args.model_name))\n",
    "    test_df = pd.read_csv(os.path.join(args.datafolder, args.test_file))\n",
    "    categories_features, numerics_features = feature_definition()\n",
    "    all_features = categories_features + numerics_features + [\"default\"]\n",
    "    print(f\"Predicting {args.test_file} ....\")\n",
    "    start_time = time.time()\n",
    "    y_test_pred = risk_model.predict_proba(test_df[all_features])\n",
    "    print(f\"Finish after {time.time() - start_time} s\")\n",
    "    y_test_pred = y_test_pred[:, 1]\n",
    "    test_df[\"default_prediction\"] = y_test_pred\n",
    "    saved_filed = os.path.join(args.datafolder, f\"{args.test_file}\")\n",
    "    print(f\"...to csv {saved_filed}\")\n",
    "    test_df.to_csv(saved_filed, index=False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893a929-c5aa-4070-9fa6-8fbad3d2fdb4",
   "metadata": {},
   "source": [
    "##### TERMINAL SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d5006c0-225a-48fe-9eab-33eb21f6c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "args.model_dir ./default_modeling/default_modeling/interface/\n",
      "args.model_name risk_model.joblib\n",
      "args.test_file test_set_2.csv\n",
      "Model path: ./default_modeling/default_modeling/interface/risk_model.joblib\n",
      "Predicting....\n",
      "Finish after 0.2402658462524414 s\n",
      "...to csv ./test_data/test_set_2.csv\n"
     ]
    }
   ],
   "source": [
    "!python3 -m default_modeling.predict --model-dir ./default_modeling/default_modeling/interface/ \\\n",
    "                                     --model-name risk_model.joblib  \\\n",
    "                                     --datafolder ./test_data \\\n",
    "                                     --test-file test_set_2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591ee3c-09c1-424b-98d1-e7f8e619840d",
   "metadata": {},
   "source": [
    "#### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8190c538-b172-4fe8-b12a-375f58fc072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting default_modeling/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile default_modeling/setup.py\n",
    "\n",
    "from distutils.core import setup\n",
    "import setuptools\n",
    "\n",
    "setup(\n",
    "    name='default_modeling',\n",
    "    version='0.0.1',\n",
    "    description=\"Default Probability Estimation Library\",\n",
    "    author=\"Linh, V. Nguyen\",\n",
    "    author_email=\"linhvietnguyen.ee@gmail.com\",\n",
    "    packages=['default_modeling', 'default_modeling.interface', 'default_modeling.utils'],\n",
    "    include_package_data=True,\n",
    "    install_requires=[\n",
    "        \"pandas>=0.25.0\",\n",
    "        \"numpy=>1.17.1\",\n",
    "        \"scikit-learn>=0.23.0\",\n",
    "        \"scipy>=0.18.1\",\n",
    "        \"category_encoders>=0.23.0\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "da70760e-a132-4e9e-a6e1-8057e6a75a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\n",
      "├── DockerFile\n",
      "├── Readme.ipynb\n",
      "├── WriteFile.ipynb\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "├── \u001b[01;34mdefault_modeling\u001b[00m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── __init__.cpython-37.pyc\n",
      "│   │   ├── __main__.cpython-37.pyc\n",
      "│   │   ├── predict.cpython-37.pyc\n",
      "│   │   └── train.cpython-37.pyc\n",
      "│   ├── \u001b[01;34mdefault_modeling\u001b[00m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   │   ├── __init__.cpython-37.pyc\n",
      "│   │   │   └── __main__.cpython-37.pyc\n",
      "│   │   ├── \u001b[01;34minterface\u001b[00m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   │   │   ├── __init__.cpython-37.pyc\n",
      "│   │   │   │   ├── predict.cpython-37.pyc\n",
      "│   │   │   │   └── train.cpython-37.pyc\n",
      "│   │   │   └── risk_model.joblib\n",
      "│   │   └── \u001b[01;34mutils\u001b[00m\n",
      "│   │       ├── __init__.py\n",
      "│   │       ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │       │   ├── __init__.cpython-37.pyc\n",
      "│   │       │   ├── load.cpython-37.pyc\n",
      "│   │       │   └── preproc.cpython-37.pyc\n",
      "│   │       ├── load.py\n",
      "│   │       └── preproc.py\n",
      "│   ├── predict.py\n",
      "│   ├── setup.py\n",
      "│   ├── \u001b[01;34mtests\u001b[00m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   │   ├── __init__.cpython-37.pyc\n",
      "│   │   │   ├── test_case_base.cpython-37.pyc\n",
      "│   │   │   └── test_data_handling.cpython-37.pyc\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   │   └── test_sample_1.csv\n",
      "│   │   ├── test_case_base.py\n",
      "│   │   └── test_data_handling.py\n",
      "│   └── train.py\n",
      "├── \u001b[01;31mnotebook.tar.gz\u001b[00m\n",
      "├── requirements.txt\n",
      "├── \u001b[01;34msrc\u001b[00m\n",
      "│   └── \u001b[01;34mthird_party\u001b[00m\n",
      "│       └── \u001b[01;34mpython-path-specification\u001b[00m\n",
      "│           ├── \u001b[01;32mCHANGES.rst\u001b[00m\n",
      "│           ├── \u001b[01;32mLICENSE\u001b[00m\n",
      "│           ├── \u001b[01;32mMANIFEST.in\u001b[00m\n",
      "│           ├── \u001b[01;32mREADME.rst\u001b[00m\n",
      "│           ├── \u001b[01;34mdoc\u001b[00m\n",
      "│           │   ├── \u001b[01;32mMakefile\u001b[00m\n",
      "│           │   └── \u001b[01;34msource\u001b[00m\n",
      "│           │       ├── \u001b[01;32mapi.rst\u001b[00m\n",
      "│           │       ├── \u001b[01;32mchanges.rst\u001b[00m\n",
      "│           │       ├── \u001b[01;32mconf.py\u001b[00m\n",
      "│           │       ├── \u001b[01;32mindex.rst\u001b[00m\n",
      "│           │       └── \u001b[01;32mreadme.rst\u001b[00m\n",
      "│           ├── \u001b[01;34mpathspec\u001b[00m\n",
      "│           │   ├── \u001b[01;32m__init__.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32m_meta.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32mcompat.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32mpathspec.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32mpattern.py\u001b[00m\n",
      "│           │   ├── \u001b[01;34mpatterns\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32m__init__.py\u001b[00m\n",
      "│           │   │   └── \u001b[01;32mgitwildmatch.py\u001b[00m\n",
      "│           │   ├── \u001b[01;34mtests\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32m__init__.py\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32mtest_gitwildmatch.py\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32mtest_pathspec.py\u001b[00m\n",
      "│           │   │   └── \u001b[01;32mtest_util.py\u001b[00m\n",
      "│           │   └── \u001b[01;32mutil.py\u001b[00m\n",
      "│           ├── \u001b[01;36mpathspec_meta.py\u001b[00m -> \u001b[01;32mpathspec/_meta.py\u001b[00m\n",
      "│           ├── \u001b[01;32mpypi-upload.sh\u001b[00m\n",
      "│           ├── \u001b[01;32msetup.cfg\u001b[00m\n",
      "│           ├── \u001b[01;32msetup.py\u001b[00m\n",
      "│           ├── \u001b[01;32mtox.ini\u001b[00m\n",
      "│           └── \u001b[01;32mtox_pip_install.py\u001b[00m\n",
      "├── \u001b[01;34mtest_data\u001b[00m\n",
      "│   ├── test_set_1.csv\n",
      "│   └── test_set_2.csv\n",
      "├── \u001b[01;34mtrain_data\u001b[00m\n",
      "│   └── train_set.csv\n",
      "└── \u001b[01;34mtutorials\u001b[00m\n",
      "    ├── \u001b[01;34mbigquery\u001b[00m\n",
      "    │   ├── \u001b[01;32mBigQuery basics.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mBigQuery command-line tool.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mBigQuery query magic.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mGetting started with BigQuery ML.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mVisualizing BigQuery public data.ipynb\u001b[00m\n",
      "    │   └── \u001b[01;34mresources\u001b[00m\n",
      "    │       ├── \u001b[01;32mmodel-evaluation.png\u001b[00m\n",
      "    │       ├── \u001b[01;32mpurchase-predictions.png\u001b[00m\n",
      "    │       ├── \u001b[01;32mtraining-statistics.png\u001b[00m\n",
      "    │       ├── \u001b[01;32mtransaction-predictions.png\u001b[00m\n",
      "    │       └── \u001b[01;32mus-states.csv\u001b[00m\n",
      "    ├── \u001b[01;34mcloud-ml-engine\u001b[00m\n",
      "    │   └── \u001b[01;32mTraining and prediction with scikit-learn.ipynb\u001b[00m\n",
      "    ├── \u001b[01;34mfairing\u001b[00m\n",
      "    │   ├── \u001b[01;32mdeploy-to-gcp.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mfairing-kubeflow-gke.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mfairing-xgboost.ipynb\u001b[00m\n",
      "    │   └── \u001b[01;32mhello-world.ipynb\u001b[00m\n",
      "    └── \u001b[01;34mstorage\u001b[00m\n",
      "        ├── \u001b[01;32mCloud Storage client library.ipynb\u001b[00m\n",
      "        ├── \u001b[01;32mStorage command-line tool.ipynb\u001b[00m\n",
      "        └── \u001b[01;34mresources\u001b[00m\n",
      "            ├── \u001b[01;32mdownloaded-us-states.txt\u001b[00m\n",
      "            └── \u001b[01;32mus-states.txt\u001b[00m\n",
      "\n",
      "29 directories, 84 files\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769eeed-e5ab-4686-85ca-b664ba2c901a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DOCKFERFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b1c925a-19ae-41f1-9283-aa4cf403d8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DockerFile\n"
     ]
    }
   ],
   "source": [
    "%%writefile DockerFile\n",
    "\n",
    "FROM python:3.8\n",
    "RUN pwd\n",
    "RUN dir\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "ENV TRAINING_FOLDER=./train_data\n",
    "ENV TESTING_FOLDER=./test_data\n",
    "ENV MODEL_DIR=./default_modeling/default_modeling/interface/\n",
    "ENV MODEL_NAME=risk_model.joblib\n",
    "\n",
    "COPY default_modeling default_modeling\n",
    "COPY train_data train_data\n",
    "\n",
    "RUN dir\n",
    "RUN python3 -m default_modeling.train --datafolder ${TRAINING_FOLDER} \\\n",
    "                                      --model-dir ${MODEL_DIR} \\\n",
    "                                      --model-name ${MODEL_NAME}\n",
    "ENTRYPOINT [\"python3\", \"-m\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3082f-a585-4d5f-a9d1-805c1b98e2cc",
   "metadata": {},
   "source": [
    "# BUILDING IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6eecba27-ff4d-465f-bdab-632346157152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  65.99MB\n",
      "Step 1/14 : FROM python:3.8\n",
      " ---> 79372a158581\n",
      "Step 2/14 : RUN pwd\n",
      " ---> Using cache\n",
      " ---> a1eb9c6ecf40\n",
      "Step 3/14 : RUN dir\n",
      " ---> Using cache\n",
      " ---> 86f013f58573\n",
      "Step 4/14 : ADD requirements.txt .\n",
      " ---> Using cache\n",
      " ---> 802ce470a44d\n",
      "Step 5/14 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> f48df72b81f6\n",
      "Step 6/14 : ENV TRAINING_FOLDER=./train_data\n",
      " ---> Using cache\n",
      " ---> ed426028a565\n",
      "Step 7/14 : ENV TESTING_FOLDER=./test_data\n",
      " ---> Using cache\n",
      " ---> cf87e3385b35\n",
      "Step 8/14 : ENV MODEL_DIR=./default_modeling/default_modeling/interface/\n",
      " ---> Using cache\n",
      " ---> 5ae07b8e658e\n",
      "Step 9/14 : ENV MODEL_NAME=risk_model.joblib\n",
      " ---> Using cache\n",
      " ---> 1a59dc9dd9ca\n",
      "Step 10/14 : ADD default_modeling default_modeling\n",
      " ---> Using cache\n",
      " ---> 026b4ed8ea6b\n",
      "Step 11/14 : ADD train_data train_data\n",
      " ---> Using cache\n",
      " ---> 6101affba3d0\n",
      "Step 12/14 : RUN dir\n",
      " ---> Using cache\n",
      " ---> 2703567123b9\n",
      "Step 13/14 : RUN python3 -m default_modeling.train --datafolder ${TRAINING_FOLDER}                                       --model-dir ${MODEL_DIR}                                       --model-name ${MODEL_NAME}\n",
      " ---> Using cache\n",
      " ---> da625c2a3508\n",
      "Step 14/14 : ENTRYPOINT [\"python3\", \"-m\"]\n",
      " ---> Running in 95aa1cf2995f\n",
      "Removing intermediate container 95aa1cf2995f\n",
      " ---> 56a8f4ca5660\n",
      "Successfully built 56a8f4ca5660\n",
      "Successfully tagged default_model:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t default_model -f DockerFile ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d1998-0556-49f8-a0b5-0929c9186e6f",
   "metadata": {},
   "source": [
    "# Run Unit Test in Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef1b8131-2ccd-4c41-8854-68432ddcd0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder view\n",
      "[PosixPath('bin'), PosixPath('sys'), PosixPath('dev'), PosixPath('var'), PosixPath('mnt'), PosixPath('opt'), PosixPath('proc'), PosixPath('boot'), PosixPath('usr'), PosixPath('lib'), PosixPath('media'), PosixPath('etc'), PosixPath('sbin'), PosixPath('root'), PosixPath('lib64'), PosixPath('srv'), PosixPath('run'), PosixPath('tmp'), PosixPath('home'), PosixPath('default_modeling'), PosixPath('train_data')]\n",
      "Found the following test data\n",
      "default_modeling/tests/data/test_sample_1.csv\n",
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.716s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m unittest discover default_modeling\n",
    "!docker run -t default_model:latest  unittest discover default_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6ec35-34db-4be4-ac77-4c99b9774b3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## USE IMAGE TO PREDICT NEW DATA 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ed30b6c9-471e-48e1-afb8-682f0fb85e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Model path: ./default_modeling/default_modeling/interface/risk_model.joblib\n",
      "Predicting test_set_1.csv ....\n",
      "Finish after 0.43554115295410156 s\n",
      "...to csv ./test_data/test_set_1.csv\n"
     ]
    }
   ],
   "source": [
    "!docker run -v /home/jupyter/test_data:/test_data default_model:latest \\\n",
    "                                     default_modeling.predict \\\n",
    "                                     --test-file test_set_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283731bc-189a-4fa0-b628-0aa73a7eb695",
   "metadata": {},
   "source": [
    "## USE IMAGE TO PREDICT NEW DATA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4f75d9bb-144d-4012-ad18-9ef8ce0cb7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Model path: ./default_modeling/default_modeling/interface/risk_model.joblib\n",
      "Predicting test_set_2.csv ....\n",
      "Finish after 0.23408913612365723 s\n",
      "...to csv ./test_data/test_set_2.csv\n"
     ]
    }
   ],
   "source": [
    "!docker run -v /home/jupyter/test_data:/test_data default_model:latest \\\n",
    "                                     default_modeling.predict \\\n",
    "                                     --test-file test_set_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89fbd27-7331-479a-8996-b949f90bf24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build_Image.ipynb\n",
      "Build_Image.md\n",
      "Build_Image_Test.ipynb\n",
      "Dockerfile\n",
      "Prototype_and_Experiment.ipynb\n",
      "Readme.ipynb\n",
      "Readme.md\n",
      "Try Cython.ipynb\n",
      "build/\n",
      "build/lib.linux-x86_64-3.7/\n",
      "build/lib.linux-x86_64-3.7/mytrain.cpython-37m-x86_64-linux-gnu.so\n",
      "build/temp.linux-x86_64-3.7/\n",
      "build/temp.linux-x86_64-3.7/mytrain.o\n",
      "build/temp.linux-x86_64-3.7/default_modeling/\n",
      "build/temp.linux-x86_64-3.7/default_modeling/train.o\n",
      "build/temp.linux-x86_64-3.7/default_modeling/predict.o\n",
      "data/\n",
      "default_modeling/\n",
      "default_modeling/setup.py\n",
      "default_modeling/__pycache__/\n",
      "default_modeling/__pycache__/predict.cpython-37.pyc\n",
      "default_modeling/__pycache__/__main__.cpython-37.pyc\n",
      "default_modeling/__pycache__/train.cpython-37.pyc\n",
      "default_modeling/__pycache__/__init__.cpython-37.pyc\n",
      "default_modeling/__pycache__/launch_predicting.cpython-37.pyc\n",
      "default_modeling/__pycache__/launch_training.cpython-37.pyc\n",
      "default_modeling/__pycache__/setup.cpython-37.pyc\n",
      "default_modeling/train.cpython-37m-x86_64-linux-gnu.so\n",
      "default_modeling/.ipynb_checkpoints/\n",
      "default_modeling/.ipynb_checkpoints/setup-checkpoint.py\n",
      "default_modeling/default_modeling/\n",
      "default_modeling/default_modeling/.ipynb_checkpoints/\n",
      "default_modeling/default_modeling/__init__.py\n",
      "default_modeling/default_modeling/utils/\n",
      "default_modeling/default_modeling/utils/__pycache__/\n",
      "default_modeling/default_modeling/utils/__pycache__/__init__.cpython-37.pyc\n",
      "default_modeling/default_modeling/utils/__pycache__/load.cpython-37.pyc\n",
      "default_modeling/default_modeling/utils/__pycache__/preproc.cpython-37.pyc\n",
      "default_modeling/default_modeling/utils/.ipynb_checkpoints/\n",
      "default_modeling/default_modeling/utils/.ipynb_checkpoints/preproc-checkpoint.py\n",
      "default_modeling/default_modeling/utils/preproc.py\n",
      "default_modeling/default_modeling/utils/__init__.py\n",
      "default_modeling/default_modeling/utils/load.py\n",
      "default_modeling/default_modeling/interface/\n",
      "default_modeling/default_modeling/interface/train.py\n",
      "default_modeling/default_modeling/interface/.ipynb_checkpoints/\n",
      "default_modeling/default_modeling/interface/.ipynb_checkpoints/__init__-checkpoint.py\n",
      "default_modeling/default_modeling/interface/.ipynb_checkpoints/predict-checkpoint.py\n",
      "default_modeling/default_modeling/interface/.ipynb_checkpoints/train-checkpoint.py\n",
      "default_modeling/default_modeling/interface/__init__.py\n",
      "default_modeling/default_modeling/interface/predict.py\n",
      "default_modeling/predict.cpython-37m-x86_64-linux-gnu.so\n",
      "default_modeling/__init__.py\n",
      "default_modeling/tests/\n",
      "default_modeling/tests/__pycache__/\n",
      "default_modeling/tests/__pycache__/test_data_handling.cpython-37.pyc\n",
      "default_modeling/tests/__pycache__/__init__.cpython-37.pyc\n",
      "default_modeling/tests/__pycache__/test_case_base.cpython-37.pyc\n",
      "default_modeling/tests/.ipynb_checkpoints/\n",
      "default_modeling/tests/.ipynb_checkpoints/test_data_handling-checkpoint.py\n",
      "default_modeling/tests/.ipynb_checkpoints/test_case_base-checkpoint.py\n",
      "default_modeling/tests/test_case_base.py\n",
      "default_modeling/tests/__init__.py\n",
      "default_modeling/tests/test_data_handling.py\n",
      "default_modeling/tests/data/\n",
      "default_modeling/tests/data/test_sample_1.csv\n",
      "model/\n",
      "model/risk_model.joblib\n",
      "mytrain.cpython-37m-x86_64-linux-gnu.so\n",
      "requirements.txt\n",
      "src/\n",
      "src/third_party/\n",
      "src/third_party/python-path-specification/\n",
      "src/third_party/python-path-specification/LICENSE\n",
      "src/third_party/python-path-specification/setup.py\n",
      "src/third_party/python-path-specification/tox.ini\n",
      "src/third_party/python-path-specification/.git/\n",
      "src/third_party/python-path-specification/.git/packed-refs\n",
      "src/third_party/python-path-specification/.git/hooks/\n",
      "src/third_party/python-path-specification/.git/hooks/fsmonitor-watchman.sample\n",
      "src/third_party/python-path-specification/.git/hooks/prepare-commit-msg.sample\n",
      "src/third_party/python-path-specification/.git/hooks/pre-rebase.sample\n",
      "src/third_party/python-path-specification/.git/hooks/commit-msg.sample\n",
      "src/third_party/python-path-specification/.git/hooks/pre-push.sample\n",
      "src/third_party/python-path-specification/.git/hooks/pre-receive.sample\n",
      "src/third_party/python-path-specification/.git/hooks/pre-applypatch.sample\n",
      "src/third_party/python-path-specification/.git/hooks/post-update.sample\n",
      "src/third_party/python-path-specification/.git/hooks/update.sample\n",
      "src/third_party/python-path-specification/.git/hooks/applypatch-msg.sample\n",
      "src/third_party/python-path-specification/.git/hooks/pre-commit.sample\n",
      "src/third_party/python-path-specification/.git/shallow\n",
      "src/third_party/python-path-specification/.git/config\n",
      "src/third_party/python-path-specification/.git/branches/\n",
      "src/third_party/python-path-specification/.git/objects/\n",
      "src/third_party/python-path-specification/.git/objects/9b/\n",
      "src/third_party/python-path-specification/.git/objects/9b/444f5232179e364b08456c1643a77f2b57569c\n",
      "src/third_party/python-path-specification/.git/objects/4d/\n",
      "src/third_party/python-path-specification/.git/objects/4d/2c74b3d03a1d3594b9ced74884f74707fb8bf4\n",
      "src/third_party/python-path-specification/.git/objects/af/\n",
      "src/third_party/python-path-specification/.git/objects/af/d8f6b4cfea559c41be58178b357862c6e07b13\n",
      "src/third_party/python-path-specification/.git/objects/09/\n",
      "src/third_party/python-path-specification/.git/objects/09/056870c0be42ca692f7837105f735873446f93\n",
      "src/third_party/python-path-specification/.git/objects/db/\n",
      "src/third_party/python-path-specification/.git/objects/db/9b08bd7ede90af68c0c10cc19ddb34c8f099a8\n",
      "src/third_party/python-path-specification/.git/objects/f5/\n",
      "src/third_party/python-path-specification/.git/objects/f5/d17bf3ced789593b955546eb12cd27f15cd31f\n",
      "src/third_party/python-path-specification/.git/objects/c3/\n",
      "src/third_party/python-path-specification/.git/objects/c3/54c2632a55e18ea0b94c5582dbed75d6212828\n",
      "src/third_party/python-path-specification/.git/objects/2e/\n",
      "src/third_party/python-path-specification/.git/objects/2e/4afd07171fa2992a35d415a071076c20c54812\n",
      "src/third_party/python-path-specification/.git/objects/84/\n",
      "src/third_party/python-path-specification/.git/objects/84/37becf4229130fbdc8863e315a1b1e1d45f1c9\n",
      "src/third_party/python-path-specification/.git/objects/pack/\n",
      "src/third_party/python-path-specification/.git/objects/35/\n",
      "src/third_party/python-path-specification/.git/objects/35/425877797818acfc9446104f1f4fbff6407793\n",
      "src/third_party/python-path-specification/.git/objects/7c/\n",
      "src/third_party/python-path-specification/.git/objects/7c/a27e2300ced6364a3816820eb2532c89c7d01c\n",
      "src/third_party/python-path-specification/.git/objects/79/\n",
      "src/third_party/python-path-specification/.git/objects/79/0d0a33bb7179331ba3ddcbb1b97ece1075af92\n",
      "src/third_party/python-path-specification/.git/objects/ae/\n",
      "src/third_party/python-path-specification/.git/objects/ae/10c9b45ea3843c09f308fcd0b2aa279ec7f7b3\n",
      "src/third_party/python-path-specification/.git/objects/26/\n",
      "src/third_party/python-path-specification/.git/objects/26/ba14111fae1325a016a306f8e1f45869cd2179\n",
      "src/third_party/python-path-specification/.git/objects/94/\n",
      "src/third_party/python-path-specification/.git/objects/94/3bde259c483951d25461a77f68cf9915296376\n",
      "src/third_party/python-path-specification/.git/objects/ff/\n",
      "src/third_party/python-path-specification/.git/objects/ff/4c3239a976e10567bb778ed95ef7e56430eed7\n",
      "src/third_party/python-path-specification/.git/objects/ff/40089d2c58a68f2c5f924b094b4b383aff5e9d\n",
      "src/third_party/python-path-specification/.git/objects/14/\n",
      "src/third_party/python-path-specification/.git/objects/14/e2f777f6c395e7e04ab4aa306bbcc4b0c1120e\n",
      "src/third_party/python-path-specification/.git/objects/98/\n",
      "src/third_party/python-path-specification/.git/objects/98/68381bee006657a5602d57ba5f902ef9487c49\n",
      "src/third_party/python-path-specification/.git/objects/d9/\n",
      "src/third_party/python-path-specification/.git/objects/d9/5853e2cc2d03b182d851c3aed4a070ace71837\n",
      "src/third_party/python-path-specification/.git/objects/7f/\n",
      "src/third_party/python-path-specification/.git/objects/7f/8c573cd82e4d5a329c20c8a711375f349368b2\n",
      "src/third_party/python-path-specification/.git/objects/f6/\n",
      "src/third_party/python-path-specification/.git/objects/f6/74265af0daf930b8e773e244022d1031ded7d6\n",
      "src/third_party/python-path-specification/.git/objects/b5/\n",
      "src/third_party/python-path-specification/.git/objects/b5/631bd95efd7b552dab6b53f374960a214dd944\n",
      "src/third_party/python-path-specification/.git/objects/18/\n",
      "src/third_party/python-path-specification/.git/objects/18/53e8ddce74c471ab6175803669bba8ac358387\n",
      "src/third_party/python-path-specification/.git/objects/39/\n",
      "src/third_party/python-path-specification/.git/objects/39/301d8798ef6938c9cc2984779e93c2109d8fb5\n",
      "src/third_party/python-path-specification/.git/objects/15/\n",
      "src/third_party/python-path-specification/.git/objects/15/ca0ddfdc32f642574beec2f123e47e314fc57d\n",
      "src/third_party/python-path-specification/.git/objects/15/f52fc01db9b1fd89f1bc7abb116244d998d1d9\n",
      "src/third_party/python-path-specification/.git/objects/1a/\n",
      "src/third_party/python-path-specification/.git/objects/1a/0d55ec74d901a6d08460e729a3ea8c321a5cd7\n",
      "src/third_party/python-path-specification/.git/objects/ee/\n",
      "src/third_party/python-path-specification/.git/objects/ee/155378521f1737f9c93a520090e94f35215dd0\n",
      "src/third_party/python-path-specification/.git/objects/info/\n",
      "src/third_party/python-path-specification/.git/objects/49/\n",
      "src/third_party/python-path-specification/.git/objects/49/46d7297cfd48e144a622199e170577017b96b3\n",
      "src/third_party/python-path-specification/.git/objects/27/\n",
      "src/third_party/python-path-specification/.git/objects/27/950ad569b98852e1997282ead835e23cb94c0c\n",
      "src/third_party/python-path-specification/.git/objects/5b/\n",
      "src/third_party/python-path-specification/.git/objects/5b/51fae3a959f627ddf542066c93284358b82a01\n",
      "src/third_party/python-path-specification/.git/objects/e8/\n",
      "src/third_party/python-path-specification/.git/objects/e8/ac70a14a3d38069e35b0b0622c6a0ff5e8560e\n",
      "src/third_party/python-path-specification/.git/objects/64/\n",
      "src/third_party/python-path-specification/.git/objects/64/a5dea9db26abf74490d9795d480f99c6cc48ea\n",
      "src/third_party/python-path-specification/.git/objects/e5/\n",
      "src/third_party/python-path-specification/.git/objects/e5/79db38c14eae0dcbc2b4e37e5b181d46cf6668\n",
      "src/third_party/python-path-specification/.git/objects/3c/\n",
      "src/third_party/python-path-specification/.git/objects/3c/e52d72d1b4c52b96eb20a0783404793b9bcf07\n",
      "src/third_party/python-path-specification/.git/objects/e6/\n",
      "src/third_party/python-path-specification/.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391\n",
      "src/third_party/python-path-specification/.git/logs/\n",
      "src/third_party/python-path-specification/.git/logs/HEAD\n",
      "src/third_party/python-path-specification/.git/logs/refs/\n",
      "src/third_party/python-path-specification/.git/logs/refs/remotes/\n",
      "src/third_party/python-path-specification/.git/logs/refs/remotes/origin/\n",
      "src/third_party/python-path-specification/.git/logs/refs/remotes/origin/HEAD\n",
      "src/third_party/python-path-specification/.git/logs/refs/heads/\n",
      "src/third_party/python-path-specification/.git/logs/refs/heads/master\n",
      "src/third_party/python-path-specification/.git/HEAD\n",
      "src/third_party/python-path-specification/.git/refs/\n",
      "src/third_party/python-path-specification/.git/refs/remotes/\n",
      "src/third_party/python-path-specification/.git/refs/remotes/origin/\n",
      "src/third_party/python-path-specification/.git/refs/remotes/origin/HEAD\n",
      "src/third_party/python-path-specification/.git/refs/tags/\n",
      "src/third_party/python-path-specification/.git/refs/tags/v0.9.0\n",
      "src/third_party/python-path-specification/.git/refs/heads/\n",
      "src/third_party/python-path-specification/.git/refs/heads/master\n",
      "src/third_party/python-path-specification/.git/info/\n",
      "src/third_party/python-path-specification/.git/info/exclude\n",
      "src/third_party/python-path-specification/.git/description\n",
      "src/third_party/python-path-specification/.git/index\n",
      "src/third_party/python-path-specification/MANIFEST.in\n",
      "src/third_party/python-path-specification/.travis.yml\n",
      "src/third_party/python-path-specification/pathspec/\n",
      "src/third_party/python-path-specification/pathspec/pattern.py\n",
      "src/third_party/python-path-specification/pathspec/__init__.py\n",
      "src/third_party/python-path-specification/pathspec/pathspec.py\n",
      "src/third_party/python-path-specification/pathspec/tests/\n",
      "src/third_party/python-path-specification/pathspec/tests/test_gitwildmatch.py\n",
      "src/third_party/python-path-specification/pathspec/tests/test_pathspec.py\n",
      "src/third_party/python-path-specification/pathspec/tests/__init__.py\n",
      "src/third_party/python-path-specification/pathspec/tests/test_util.py\n",
      "src/third_party/python-path-specification/pathspec/patterns/\n",
      "src/third_party/python-path-specification/pathspec/patterns/__init__.py\n",
      "src/third_party/python-path-specification/pathspec/patterns/gitwildmatch.py\n",
      "src/third_party/python-path-specification/pathspec/util.py\n",
      "src/third_party/python-path-specification/pathspec/compat.py\n",
      "src/third_party/python-path-specification/pathspec/_meta.py\n",
      "src/third_party/python-path-specification/setup.cfg\n",
      "src/third_party/python-path-specification/README.rst\n",
      "src/third_party/python-path-specification/tox_pip_install.py\n",
      "src/third_party/python-path-specification/CHANGES.rst\n",
      "src/third_party/python-path-specification/doc/\n",
      "src/third_party/python-path-specification/doc/Makefile\n",
      "src/third_party/python-path-specification/doc/source/\n",
      "src/third_party/python-path-specification/doc/source/readme.rst\n",
      "src/third_party/python-path-specification/doc/source/changes.rst\n",
      "src/third_party/python-path-specification/doc/source/conf.py\n",
      "src/third_party/python-path-specification/doc/source/index.rst\n",
      "src/third_party/python-path-specification/doc/source/api.rst\n",
      "src/third_party/python-path-specification/pathspec_meta.py\n",
      "src/third_party/python-path-specification/.gitignore\n",
      "src/third_party/python-path-specification/pypi-upload.sh\n",
      "test_data/\n",
      "test_data/test_set_2.csv\n",
      "test_data/.ipynb_checkpoints/\n",
      "test_data/.ipynb_checkpoints/test_set_1-checkpoint.csv\n",
      "test_data/.ipynb_checkpoints/test_set_2-checkpoint.csv\n",
      "test_data/test_set_1.csv\n",
      "train.cpython-37m-x86_64-linux-gnu.so\n",
      "train_data/\n",
      "train_data/.ipynb_checkpoints/\n",
      "train_data/.ipynb_checkpoints/train_set_2-checkpoint.csv\n",
      "train_data/train_set_1.csv\n",
      "train_data/train_set_2.csv\n",
      "tutorials/\n",
      "tutorials/fairing/\n",
      "tutorials/fairing/fairing-xgboost.ipynb\n",
      "tutorials/fairing/hello-world.ipynb\n",
      "tutorials/fairing/deploy-to-gcp.ipynb\n",
      "tutorials/fairing/fairing-kubeflow-gke.ipynb\n",
      "tutorials/bigquery/\n",
      "tutorials/bigquery/BigQuery basics.ipynb\n",
      "tutorials/bigquery/Getting started with BigQuery ML.ipynb\n",
      "tutorials/bigquery/BigQuery query magic.ipynb\n",
      "tutorials/bigquery/Visualizing BigQuery public data.ipynb\n",
      "tutorials/bigquery/BigQuery command-line tool.ipynb\n",
      "tutorials/bigquery/resources/\n",
      "tutorials/bigquery/resources/model-evaluation.png\n",
      "tutorials/bigquery/resources/us-states.csv\n",
      "tutorials/bigquery/resources/transaction-predictions.png\n",
      "tutorials/bigquery/resources/purchase-predictions.png\n",
      "tutorials/bigquery/resources/training-statistics.png\n",
      "tutorials/cloud-ml-engine/\n",
      "tutorials/cloud-ml-engine/Training and prediction with scikit-learn.ipynb\n",
      "tutorials/storage/\n",
      "tutorials/storage/Cloud Storage client library.ipynb\n",
      "tutorials/storage/Storage command-line tool.ipynb\n",
      "tutorials/storage/resources/\n",
      "tutorials/storage/resources/us-states.txt\n",
      "tutorials/storage/resources/downloaded-us-states.txt\n"
     ]
    }
   ],
   "source": [
    "!tar chvfz notebook.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e65a00e9-0212-4b69-9c84-41e744f3b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n",
      "[NbConvertApp] Converting notebook Build_Image.ipynb to markdown\n",
      "[NbConvertApp] Writing 8954 bytes to Build_Image.md\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to markdown Build_Image.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cdb91e-1899-486e-9585-9caa454e189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting DockerFile\n"
     ]
    }
   ],
   "source": [
    "%%writefile DockerFile\n",
    "\n",
    "FROM python:3.8\n",
    "WORKDIR /app/\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY default_modeling default_modeling\n",
    "COPY train_data train_data\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9481d8cd-9b61-4593-9c2f-39107a32d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   71.2MB\n",
      "Step 1/7 : FROM python:3.8\n",
      " ---> 79372a158581\n",
      "Step 2/7 : WORKDIR /app/\n",
      " ---> Using cache\n",
      " ---> 6927edeea50a\n",
      "Step 3/7 : COPY requirements.txt .\n",
      " ---> Using cache\n",
      " ---> be3191584c0c\n",
      "Step 4/7 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> b4606af6a945\n",
      "Step 5/7 : COPY default_modeling default_modeling\n",
      " ---> 1d3798217e7f\n",
      "Step 6/7 : COPY train_data train_data\n",
      " ---> 3874f13479f2\n",
      "Step 7/7 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in 286483df009c\n",
      "Removing intermediate container 286483df009c\n",
      " ---> d952a8249917\n",
      "Successfully built d952a8249917\n",
      "Successfully tagged default_model:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t default_model -f DockerFile ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cfc4a-af2b-412e-85f3-cc3cb9a08509",
   "metadata": {},
   "source": [
    "### RUN TRAINING\n",
    "- ENV TRAINING_FOLDER=./train_data\n",
    "- ENV TESTING_FOLDER=./test_data\n",
    "- ENV MODEL_DIR=./default_modeling/default_modeling/interface/\n",
    "- ENV MODEL_NAME=risk_model.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c7c9de-e54f-45a4-8fd1-a78b25bdc8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env\n",
    "TRAINING_FOLDER=./train_data\n",
    "TESTING_FOLDER=./test_data\n",
    "MODEL_DIR=./default_modeling/default_modeling/interface/\n",
    "MODEL_NAME=risk_model.joblib\n",
    "N_ESTIMATORS=5\n",
    "MIN_SAMPLES_LEAF=5\n",
    "MAX_DEPTH=5\n",
    "RANDOM_STATE=123\n",
    "TARGET=default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2040bb-8a4d-41c5-8ed8-03b5338d9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --env-file .env -v /home/jupyter/train_data:/app/train_data default_model:latest  \\\n",
    "                                                        -m default_modeling.train \\\n",
    "                                                        --n-estimators .env[\"N_ESTIMATORS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe2c250-068e-4a80-a564-01f89f1d7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "Model path: ./default_modeling/default_modeling/interface/risk_model.joblib\n",
      "Predicting test_set_1.csv ....\n",
      "Finish after 0.4289968013763428 s\n",
      "...to csv ./test_data/test_set_1.csv\n"
     ]
    }
   ],
   "source": [
    "!docker run -v /home/jupyter/test_data:/app/test_data default_model:latest \\\n",
    "                                     default_modeling.predict \\\n",
    "                                     --test-file test_set_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147b184a-d60b-49d0-ac8d-e956081a2bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\n",
      "├── Build_Image.ipynb\n",
      "├── Build_Image.md\n",
      "├── Build_Image_Test.ipynb\n",
      "├── Dockerfile\n",
      "├── Prototype_and_Experiment.ipynb\n",
      "├── README.md\n",
      "├── Readme.ipynb\n",
      "├── Readme.md\n",
      "├── Try Cython.ipynb\n",
      "├── \u001b[01;34mbuild\u001b[00m\n",
      "│   ├── \u001b[01;34mlib.linux-x86_64-3.7\u001b[00m\n",
      "│   │   └── \u001b[01;32mmytrain.cpython-37m-x86_64-linux-gnu.so\u001b[00m\n",
      "│   └── \u001b[01;34mtemp.linux-x86_64-3.7\u001b[00m\n",
      "│       ├── \u001b[01;34mdefault_modeling\u001b[00m\n",
      "│       │   ├── predict.o\n",
      "│       │   └── train.o\n",
      "│       └── mytrain.o\n",
      "├── \u001b[01;34mdata\u001b[00m\n",
      "├── \u001b[01;34mdefault_modeling\u001b[00m\n",
      "│   ├── __init__.py\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   ├── __init__.cpython-37.pyc\n",
      "│   │   ├── __main__.cpython-37.pyc\n",
      "│   │   ├── launch_predicting.cpython-37.pyc\n",
      "│   │   ├── launch_training.cpython-37.pyc\n",
      "│   │   ├── predict.cpython-37.pyc\n",
      "│   │   ├── setup.cpython-37.pyc\n",
      "│   │   └── train.cpython-37.pyc\n",
      "│   ├── \u001b[01;34mdefault_modeling\u001b[00m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34minterface\u001b[00m\n",
      "│   │   │   ├── __init__.py\n",
      "│   │   │   ├── predict.py\n",
      "│   │   │   └── train.py\n",
      "│   │   └── \u001b[01;34mutils\u001b[00m\n",
      "│   │       ├── __init__.py\n",
      "│   │       ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │       │   ├── __init__.cpython-37.pyc\n",
      "│   │       │   ├── load.cpython-37.pyc\n",
      "│   │       │   └── preproc.cpython-37.pyc\n",
      "│   │       ├── load.py\n",
      "│   │       └── preproc.py\n",
      "│   ├── \u001b[01;32mpredict.cpython-37m-x86_64-linux-gnu.so\u001b[00m\n",
      "│   ├── setup.py\n",
      "│   ├── \u001b[01;34mtests\u001b[00m\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── \u001b[01;34m__pycache__\u001b[00m\n",
      "│   │   │   ├── __init__.cpython-37.pyc\n",
      "│   │   │   ├── test_case_base.cpython-37.pyc\n",
      "│   │   │   └── test_data_handling.cpython-37.pyc\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   │   └── test_sample_1.csv\n",
      "│   │   ├── test_case_base.py\n",
      "│   │   └── test_data_handling.py\n",
      "│   └── \u001b[01;32mtrain.cpython-37m-x86_64-linux-gnu.so\u001b[00m\n",
      "├── \u001b[01;34mmodel\u001b[00m\n",
      "│   └── risk_model.joblib\n",
      "├── \u001b[01;32mmytrain.cpython-37m-x86_64-linux-gnu.so\u001b[00m\n",
      "├── \u001b[01;31mnotebook.tar.gz\u001b[00m\n",
      "├── requirements.txt\n",
      "├── \u001b[01;34msrc\u001b[00m\n",
      "│   └── \u001b[01;34mthird_party\u001b[00m\n",
      "│       └── \u001b[01;34mpython-path-specification\u001b[00m\n",
      "│           ├── \u001b[01;32mCHANGES.rst\u001b[00m\n",
      "│           ├── \u001b[01;32mLICENSE\u001b[00m\n",
      "│           ├── \u001b[01;32mMANIFEST.in\u001b[00m\n",
      "│           ├── \u001b[01;32mREADME.rst\u001b[00m\n",
      "│           ├── \u001b[01;34mdoc\u001b[00m\n",
      "│           │   ├── \u001b[01;32mMakefile\u001b[00m\n",
      "│           │   └── \u001b[01;34msource\u001b[00m\n",
      "│           │       ├── \u001b[01;32mapi.rst\u001b[00m\n",
      "│           │       ├── \u001b[01;32mchanges.rst\u001b[00m\n",
      "│           │       ├── \u001b[01;32mconf.py\u001b[00m\n",
      "│           │       ├── \u001b[01;32mindex.rst\u001b[00m\n",
      "│           │       └── \u001b[01;32mreadme.rst\u001b[00m\n",
      "│           ├── \u001b[01;34mpathspec\u001b[00m\n",
      "│           │   ├── \u001b[01;32m__init__.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32m_meta.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32mcompat.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32mpathspec.py\u001b[00m\n",
      "│           │   ├── \u001b[01;32mpattern.py\u001b[00m\n",
      "│           │   ├── \u001b[01;34mpatterns\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32m__init__.py\u001b[00m\n",
      "│           │   │   └── \u001b[01;32mgitwildmatch.py\u001b[00m\n",
      "│           │   ├── \u001b[01;34mtests\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32m__init__.py\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32mtest_gitwildmatch.py\u001b[00m\n",
      "│           │   │   ├── \u001b[01;32mtest_pathspec.py\u001b[00m\n",
      "│           │   │   └── \u001b[01;32mtest_util.py\u001b[00m\n",
      "│           │   └── \u001b[01;32mutil.py\u001b[00m\n",
      "│           ├── \u001b[01;36mpathspec_meta.py\u001b[00m -> \u001b[01;32mpathspec/_meta.py\u001b[00m\n",
      "│           ├── \u001b[01;32mpypi-upload.sh\u001b[00m\n",
      "│           ├── \u001b[01;32msetup.cfg\u001b[00m\n",
      "│           ├── \u001b[01;32msetup.py\u001b[00m\n",
      "│           ├── \u001b[01;32mtox.ini\u001b[00m\n",
      "│           └── \u001b[01;32mtox_pip_install.py\u001b[00m\n",
      "├── \u001b[01;34mtest_data\u001b[00m\n",
      "│   ├── test_set_1.csv\n",
      "│   └── test_set_2.csv\n",
      "├── \u001b[01;32mtrain.cpython-37m-x86_64-linux-gnu.so\u001b[00m\n",
      "├── \u001b[01;34mtrain_data\u001b[00m\n",
      "│   ├── train_set_1.csv\n",
      "│   └── train_set_2.csv\n",
      "└── \u001b[01;34mtutorials\u001b[00m\n",
      "    ├── \u001b[01;34mbigquery\u001b[00m\n",
      "    │   ├── \u001b[01;32mBigQuery basics.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mBigQuery command-line tool.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mBigQuery query magic.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mGetting started with BigQuery ML.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mVisualizing BigQuery public data.ipynb\u001b[00m\n",
      "    │   └── \u001b[01;34mresources\u001b[00m\n",
      "    │       ├── \u001b[01;32mmodel-evaluation.png\u001b[00m\n",
      "    │       ├── \u001b[01;32mpurchase-predictions.png\u001b[00m\n",
      "    │       ├── \u001b[01;32mtraining-statistics.png\u001b[00m\n",
      "    │       ├── \u001b[01;32mtransaction-predictions.png\u001b[00m\n",
      "    │       └── \u001b[01;32mus-states.csv\u001b[00m\n",
      "    ├── \u001b[01;34mcloud-ml-engine\u001b[00m\n",
      "    │   └── \u001b[01;32mTraining and prediction with scikit-learn.ipynb\u001b[00m\n",
      "    ├── \u001b[01;34mfairing\u001b[00m\n",
      "    │   ├── \u001b[01;32mdeploy-to-gcp.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mfairing-kubeflow-gke.ipynb\u001b[00m\n",
      "    │   ├── \u001b[01;32mfairing-xgboost.ipynb\u001b[00m\n",
      "    │   └── \u001b[01;32mhello-world.ipynb\u001b[00m\n",
      "    └── \u001b[01;34mstorage\u001b[00m\n",
      "        ├── \u001b[01;32mCloud Storage client library.ipynb\u001b[00m\n",
      "        ├── \u001b[01;32mStorage command-line tool.ipynb\u001b[00m\n",
      "        └── \u001b[01;34mresources\u001b[00m\n",
      "            ├── \u001b[01;32mdownloaded-us-states.txt\u001b[00m\n",
      "            └── \u001b[01;32mus-states.txt\u001b[00m\n",
      "\n",
      "32 directories, 97 files\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c8c56-8820-4ecc-9689-f66102ed158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    .\n",
    "    ├── Dockerfile\n",
    "    ├── Prototype_and_Experiment.ipynb\n",
    "    ├── README.md\n",
    "    ├── default_modeling\n",
    "    │   ├── __init__.py\n",
    "    │   ├── default_modeling\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── interface\n",
    "    │   │   │   ├── __init__.py\n",
    "    │   │   │   ├── predict.py\n",
    "    │   │   │   └── train.py\n",
    "    │   │   └── utils\n",
    "    │   │       ├── __init__.py\n",
    "    │   │       ├── load.py\n",
    "    │   │       └── preproc.py\n",
    "    │   ├── setup.py\n",
    "    │   ├── tests\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── data\n",
    "    │   │   │   └── test_sample_1.csv\n",
    "    │   │   ├── test_case_base.py\n",
    "    │   │   └── test_data_handling.py\n",
    "    ├── model\n",
    "    │   └── risk_model.joblib\n",
    "    ├── requirements.txt\n",
    "    ├── test_data\n",
    "    │   ├── test_set_1.csv\n",
    "    │   └── test_set_2.csv\n",
    "    └── train_data\n",
    "        ├── train_set_1.csv\n",
    "        └── train_set_2.csv"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
